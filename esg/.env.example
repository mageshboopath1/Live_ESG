# ============================================================================
# ESG Intelligence Platform - Environment Configuration Template
# ============================================================================
# 
# This file contains all environment variables required to run the platform.
# Copy this file to .env and fill in your actual values.
#
# SECURITY NOTES FOR PRODUCTION:
# - NEVER commit .env files with real credentials to version control
# - Use strong, randomly generated passwords (minimum 32 characters)
# - Rotate credentials regularly (every 90 days recommended)
# - Use secrets management tools (AWS Secrets Manager, HashiCorp Vault, etc.)
# - Enable SSL/TLS for all database and service connections
# - Restrict network access using firewalls and security groups
# - Use environment-specific credentials (dev, staging, prod)
# ============================================================================

# ============================================================================
# PostgreSQL Database Configuration
# ============================================================================
# PostgreSQL is used for storing company data, embeddings, indicators, and scores
# with pgvector extension for semantic search capabilities.

# Database connection settings
POSTGRES_HOST=postgres                    # Hostname (use 'localhost' for local dev, service name for Docker)
POSTGRES_PORT=5432                        # PostgreSQL port (default: 5432)
POSTGRES_DB=esg_platform                  # Database name
POSTGRES_USER=esg_user                    # Database user
POSTGRES_PASSWORD=change_this_password    # Database password

# PRODUCTION SECURITY NOTES:
# - Use a strong password with mixed case, numbers, and special characters
# - Create separate users with limited privileges for each service
# - Enable SSL mode: add ?sslmode=require to connection strings
# - Restrict access by IP address in pg_hba.conf
# - Enable connection pooling (pgbouncer) for better performance

# Database connection aliases (used by different services)
DB_HOST=${POSTGRES_HOST}
DB_PORT=${POSTGRES_PORT}
DB_NAME=${POSTGRES_DB}
DB_USER=${POSTGRES_USER}
DB_PASSWORD=${POSTGRES_PASSWORD}

# ============================================================================
# MinIO Object Storage Configuration
# ============================================================================
# MinIO provides S3-compatible object storage for PDF documents and reports.

MINIO_ENDPOINT=http://minio:9000          # MinIO endpoint URL (include protocol)
MINIO_ROOT_USER=esg_minio_admin           # MinIO root username (like AWS access key)
MINIO_ROOT_PASSWORD=change_this_secret    # MinIO root password (like AWS secret key)
MINIO_BUCKET=esg-reports                  # Default bucket name for reports
MINIO_SECURE=false                        # Use HTTPS (set to 'true' in production)

# MinIO connection aliases (used by different services)
MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
BUCKET_NAME=${MINIO_BUCKET}

# Additional bucket names for different document types
MINIO_BUCKET_NEWS=esg-news                # News articles bucket
MINIO_BUCKET_CSR=esg-csr                  # CSR reports bucket
MINIO_BUCKET_POLLUTION=esg-pollution      # Pollution data bucket

# PRODUCTION SECURITY NOTES:
# - Use HTTPS (MINIO_SECURE=true) with valid SSL certificates
# - Create separate access keys for each service with minimal permissions
# - Enable bucket versioning for data recovery
# - Configure bucket policies to restrict public access
# - Enable server-side encryption for data at rest
# - Use presigned URLs with short expiration times for document access

# ============================================================================
# RabbitMQ Message Queue Configuration
# ============================================================================
# RabbitMQ handles asynchronous task processing for embeddings and extraction.

RABBITMQ_HOST=rabbitmq                    # RabbitMQ hostname
RABBITMQ_PORT=5672                        # AMQP port (default: 5672)
RABBITMQ_MANAGEMENT_PORT=15672            # Management UI port (default: 15672)
RABBITMQ_DEFAULT_USER=esg_rabbitmq        # RabbitMQ username
RABBITMQ_DEFAULT_PASS=change_this_pass    # RabbitMQ password

# Queue names for different processing stages
QUEUE_NAME=embedding-tasks                # Queue for embedding generation tasks
EXTRACTION_QUEUE_NAME=extraction-tasks    # Queue for indicator extraction tasks

# PRODUCTION SECURITY NOTES:
# - Use strong passwords for RabbitMQ users
# - Enable SSL/TLS for AMQP connections
# - Disable guest user in production
# - Create separate users with specific permissions for each service
# - Restrict management UI access by IP address
# - Enable message persistence for critical queues
# - Configure dead letter exchanges for failed messages

# ============================================================================
# Google Generative AI Configuration
# ============================================================================
# Google GenAI is used for embeddings (gemini-embedding-001) and 
# indicator extraction (gemini-2.5-flash) with LangChain integration.

GOOGLE_API_KEY=your_google_api_key_here   # Google AI API key from Google AI Studio
GENAI_API_KEY=${GOOGLE_API_KEY}           # Alias for backward compatibility

# PRODUCTION SECURITY NOTES:
# - Obtain API key from https://makersuite.google.com/app/apikey
# - Store API keys in secrets manager, not in environment files
# - Rotate API keys regularly
# - Monitor API usage and set up billing alerts
# - Implement rate limiting to prevent excessive API costs
# - Use separate API keys for dev, staging, and production environments

# ============================================================================
# Redis Cache Configuration
# ============================================================================
# Redis provides caching for API responses to improve performance.

REDIS_HOST=redis                          # Redis hostname
REDIS_PORT=6379                           # Redis port (default: 6379)
REDIS_DB=0                                # Redis database number (0-15)
REDIS_PASSWORD=                           # Redis password (leave empty for no auth in dev)
REDIS_ENABLED=true                        # Enable/disable caching (true/false)

# Cache TTL settings (in seconds)
CACHE_TTL_COMPANY=3600                    # Company data cache TTL (1 hour)
CACHE_TTL_INDICATORS=86400                # Indicator definitions cache TTL (24 hours)
CACHE_TTL_SCORES=3600                     # ESG scores cache TTL (1 hour)

# PRODUCTION SECURITY NOTES:
# - Always set a strong REDIS_PASSWORD in production
# - Enable SSL/TLS for Redis connections
# - Restrict Redis access by IP address
# - Use Redis ACLs to limit command access
# - Configure maxmemory and eviction policies
# - Enable persistence (AOF or RDB) for critical cached data

# ============================================================================
# API Gateway Configuration
# ============================================================================
# FastAPI gateway provides REST API for frontend and external integrations.

# Application settings
APP_NAME="ESG Intelligence Platform API"  # Application name
DEBUG=false                               # Debug mode (set to 'false' in production)
HOST=0.0.0.0                              # Bind address (0.0.0.0 for all interfaces)
PORT=8000                                 # API server port

# CORS settings (comma-separated list of allowed origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# JWT Authentication settings
SECRET_KEY=change_this_to_random_64_char_string  # JWT signing key (CRITICAL: use strong random key)
ALGORITHM=HS256                           # JWT algorithm (HS256 recommended)
ACCESS_TOKEN_EXPIRE_MINUTES=30            # Token expiration time in minutes

# Rate limiting settings
RATE_LIMIT_REQUESTS=100                   # Maximum requests per window
RATE_LIMIT_WINDOW=60                      # Rate limit window in seconds

# PRODUCTION SECURITY NOTES:
# - Generate SECRET_KEY using: openssl rand -hex 32
# - Set DEBUG=false in production to prevent information leakage
# - Configure CORS_ORIGINS to only include trusted domains
# - Use HTTPS only in production
# - Implement API key authentication for external clients
# - Enable request logging and monitoring
# - Set up rate limiting per API key/user
# - Use JWT refresh tokens for better security

# ============================================================================
# Extraction Service Configuration
# ============================================================================
# Extraction service uses LangChain and Google GenAI to extract BRSR indicators.

# Model configuration
EXTRACTION_MODEL=gemini-2.0-flash-exp     # Google model for extraction
EXTRACTION_EMBEDDING_MODEL=gemini-embedding-001  # Embedding model for retrieval
EXTRACTION_EMBEDDING_DIMENSIONS=3072      # Must match EMBEDDING_DIMENSIONS

# Processing configuration
EXTRACTION_BATCH_SIZE=10                  # Number of indicators to process per batch
CHECK_EMBEDDINGS_BEFORE_EXTRACTION=true   # Check if embeddings exist before extraction

# Retry configuration
EXTRACTION_MAX_RETRIES=3                  # Maximum retry attempts for failed extractions
EXTRACTION_RETRY_DELAY=1.0                # Initial retry delay in seconds
EXTRACTION_RETRY_BACKOFF=2.0              # Retry backoff multiplier (exponential)

# Embedding check configuration
EMBEDDING_CHECK_MAX_ATTEMPTS=10           # Maximum attempts to check for embeddings
EMBEDDING_CHECK_DELAY=30                  # Delay between embedding checks (seconds)

# Service configuration
LOG_LEVEL=INFO                            # Logging level (DEBUG, INFO, WARNING, ERROR)
MAX_RETRIES=3                             # Maximum retry attempts (legacy, use EXTRACTION_MAX_RETRIES)
INITIAL_RETRY_DELAY=1.0                   # Initial retry delay (legacy, use EXTRACTION_RETRY_DELAY)

# Monitoring configuration
HEALTH_PORT=8080                          # Health check endpoint port

# LangChain configuration
LANGCHAIN_TRACING_V2=false                # Enable LangSmith tracing (optional)
LANGCHAIN_API_KEY=                        # LangSmith API key (optional, for debugging)
LANGCHAIN_PROJECT=esg-extraction          # LangSmith project name (optional)

# PRODUCTION NOTES:
# - Set LOG_LEVEL=WARNING or ERROR in production to reduce log volume
# - Configure centralized logging (ELK, CloudWatch, etc.)
# - Monitor extraction success rates and confidence scores
# - Set up alerts for high failure rates
# - Implement circuit breakers for external API calls
# - EXTRACTION_EMBEDDING_DIMENSIONS MUST match EMBEDDING_DIMENSIONS

# ============================================================================
# Embeddings Service Configuration
# ============================================================================
# Embeddings service generates vector embeddings using Google Gemini.

# Chunking configuration
CHUNK_SIZE=1500                           # Text chunk size in characters (1000-2000 recommended)
CHUNK_OVERLAP=200                         # Overlap between chunks in characters
MAX_CHUNKS_PER_DOCUMENT=1000              # Maximum chunks per document (safety limit)

# Embedding configuration
EMBEDDING_MODEL=gemini-embedding-001      # Google embedding model name
EMBEDDING_DIMENSIONS=3072                 # Embedding vector dimensions (768 or 3072)
EMBEDDING_BATCH_SIZE=32                   # Batch size for embedding generation

# Retry configuration
EMBEDDING_MAX_RETRIES=3                   # Maximum retry attempts for failed embeddings
EMBEDDING_RETRY_DELAY=1.0                 # Initial retry delay in seconds
EMBEDDING_RETRY_BACKOFF=2.0               # Retry backoff multiplier (exponential)

# PRODUCTION NOTES:
# - Monitor embedding generation costs
# - Implement batch processing for efficiency
# - Set up retry logic for API failures
# - Track processing time per document
# - EMBEDDING_DIMENSIONS must match model output (gemini-embedding-001 = 3072)

# ============================================================================
# PgAdmin Configuration
# ============================================================================
# PgAdmin provides web-based database management interface.

PGADMIN_DEFAULT_EMAIL=admin@admin.com     # PgAdmin login email
PGADMIN_DEFAULT_PASSWORD=admin            # PgAdmin login password
PGADMIN_CONFIG_SERVER_MODE=False          # Server mode (False for single-user)
PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False  # Require master password

# PRODUCTION SECURITY NOTES:
# - Use strong password for PgAdmin access
# - Restrict PgAdmin access by IP address or VPN
# - Disable PgAdmin in production or use read-only access
# - Enable HTTPS for PgAdmin interface

# ============================================================================
# Frontend Configuration
# ============================================================================
# Vue 3 frontend application settings.

# API endpoint
VITE_API_BASE_URL=http://localhost:8000   # API gateway base URL
VITE_API_TIMEOUT=30000                    # API request timeout in milliseconds

# Feature flags
VITE_ENABLE_ANALYTICS=false               # Enable analytics tracking
VITE_ENABLE_DEBUG=false                   # Enable debug mode

# PRODUCTION NOTES:
# - Use production API URL (HTTPS)
# - Enable analytics in production
# - Disable debug mode in production
# - Configure CDN for static assets

# ============================================================================
# Ingestion Service Configuration
# ============================================================================
# Ingestion service downloads PDFs and publishes queue messages.

# Task delay configuration
EXTRACTION_TASK_DELAY=300                 # Delay before extraction tasks (seconds, 5 minutes)

# Retry configuration
INGESTION_MAX_RETRIES=3                   # Maximum retry attempts for failed downloads
INGESTION_RETRY_DELAY=1.0                 # Initial retry delay in seconds

# Processing limits
MAX_CONCURRENT_DOWNLOADS=5                # Maximum concurrent PDF downloads

# PRODUCTION NOTES:
# - EXTRACTION_TASK_DELAY should be long enough for embeddings to complete
# - For large documents, increase delay to 600-900 seconds
# - Monitor download success rates and adjust retries accordingly

# ============================================================================
# Pipeline Orchestration Configuration
# ============================================================================
# Settings for end-to-end pipeline testing and monitoring.

# Queue monitoring timeouts (in seconds)
EMBEDDING_TIMEOUT=3600                    # Embedding stage timeout (1 hour)
EXTRACTION_TIMEOUT=7200                   # Extraction stage timeout (2 hours)

# Queue monitoring intervals (in seconds)
QUEUE_CHECK_INTERVAL=10                   # How often to check queue depth
EMPTY_QUEUE_WAIT=30                       # How long queue must stay empty to confirm completion
PROGRESS_LOG_INTERVAL=60                  # How often to log progress

# RabbitMQ connection settings
RABBITMQ_HEARTBEAT=600                    # Connection heartbeat interval (seconds)
RABBITMQ_BLOCKED_TIMEOUT=300              # Blocked connection timeout (seconds)

# Google AI rate limiting
GOOGLE_API_RPM=60                         # Requests per minute limit
GOOGLE_API_MAX_CONCURRENT=5               # Maximum concurrent API requests

# PRODUCTION NOTES:
# - Adjust timeouts based on dataset size:
#   * Small (1-10 docs): EMBEDDING_TIMEOUT=600, EXTRACTION_TIMEOUT=1200
#   * Medium (10-50 docs): EMBEDDING_TIMEOUT=3600, EXTRACTION_TIMEOUT=7200
#   * Large (50+ docs): EMBEDDING_TIMEOUT=7200, EXTRACTION_TIMEOUT=14400
# - Shorter QUEUE_CHECK_INTERVAL provides more responsive monitoring
# - EMPTY_QUEUE_WAIT prevents race conditions with new messages
# - Monitor API usage to avoid rate limits and unexpected costs

# ============================================================================
# Docker Compose Configuration
# ============================================================================
# Settings for Docker Compose orchestration.

COMPOSE_PROJECT_NAME=esg-platform         # Docker Compose project name
COMPOSE_FILE=docker-compose.yml           # Compose file to use

# ============================================================================
# Development vs Production Configuration
# ============================================================================
# 
# DEVELOPMENT DEFAULTS (current values):
# - Weak passwords for easy local setup
# - Debug mode enabled
# - No SSL/TLS
# - Permissive CORS
# - Local hostnames
#
# PRODUCTION CHECKLIST:
# ✓ Generate strong random passwords for all services
# ✓ Set DEBUG=false
# ✓ Enable SSL/TLS for all connections (HTTPS, PostgreSQL SSL, Redis SSL)
# ✓ Configure restrictive CORS origins
# ✓ Use secrets management (AWS Secrets Manager, Vault, etc.)
# ✓ Enable authentication and authorization
# ✓ Set up monitoring and alerting
# ✓ Configure backup and disaster recovery
# ✓ Implement rate limiting and DDoS protection
# ✓ Use separate credentials for each environment
# ✓ Enable audit logging
# ✓ Restrict network access with firewalls
# ✓ Use container orchestration (Kubernetes, ECS) instead of Docker Compose
# ✓ Implement health checks and auto-scaling
# ✓ Configure log aggregation and analysis
#
# ============================================================================
